{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for the TF RNN cell\n",
    "# For an LSTM, the 'cell' is a tuple containing state and cell\n",
    "# We use TF's dropout to implement zoneout\n",
    "class ZoneoutWrapper(tf.nn.rnn_cell.RNNCell):\n",
    "  \n",
    "  #Operator adding zoneout to all states (states+cells) of the given cell.\n",
    "  def __init__(self, cell, drop_prob, is_training=True, seed=0):\n",
    "    self._cell = cell\n",
    "    self._keep_prob = 1 - drop_prob\n",
    "    self._seed = seed\n",
    "    self.is_training = is_training\n",
    "  \n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cell.output_size\n",
    "\n",
    "  def __call__(self, inputs, state):\n",
    "    if isinstance(self.state_size, tuple) != isinstance(self._zoneout_prob, tuple):\n",
    "      raise TypeError('Subdivided states need subdivided zoneouts.')\n",
    "    if isinstance(self.state_size, tuple) and len(tuple(self.state_size)) != len(tuple(self._zoneout_prob)):\n",
    "      raise ValueError('State and zoneout need equally many parts.')\n",
    "    output, new_state = self._cell(inputs, state, scope)\n",
    "    if self.is_training:\n",
    "      new_state = self._keep_prob * tf.nn.dropout(new_state - state, self._keep_prob, seed = self._seed) + state\n",
    "    else:\n",
    "      new_state = self._keep_prob * new_state + (1 - self._keep_prob) * state\n",
    "    return output, new_state\n",
    "\n",
    "\n",
    "# Wrap your cells like this\n",
    "#cell = ZoneoutWrapper(cell, keep_prob=(z_prob_cells, z_prob_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePredFeature:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            #Generate placeholder variables to represent the input tensors\n",
    "            self.inputs_placeholder = tf.placeholder(tf.int32, shape=(None, \n",
    "                                            self.config.max_length, self.config.feature_size), name=\"x\")\n",
    "            self.labels_placeholder = tf.placeholder(tf.int32, shape=(None, self.config.max_length), name=\"y\")\n",
    "            self.dropout_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "            #Creates one-hot encoding for the input. No embedding is used as of now\n",
    "            batch_size = tf.shape(self.inputs_placeholder)[0]\n",
    "            embedding = tf.one_hot(self.inputs_placeholder, self.config.num_classes)\n",
    "            embedding = tf.reshape(embedding, [self.config.batch_size, self.config.max_length, \n",
    "                                               self.config.feature_size * self.config.num_classes])\n",
    "\n",
    "            self.pred = self.add_prediction_op()\n",
    "            self.loss = self.add_loss_op(self.pred)\n",
    "            self.global_step, self.train_op = self.add_training_op(self.loss)\n",
    "            self.merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None, initial_state=None, keep_prob=1.0):\n",
    "        \"\"\"Creates the feed_dict for the model.\n",
    "        NOTE: You do not have to do anything here.\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.inputs_placeholder: inputs_batch,\n",
    "            self.dropout_placeholder: keep_prob,\n",
    "            }\n",
    "        if labels_batch is not None:\n",
    "            feed_dict[self.labels_placeholder] = labels_batch\n",
    "\n",
    "        if initial_state is not None:\n",
    "            feed_dict[self.in_state] = initial_state\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def add_embedding(self):\n",
    "\n",
    "        \"\"\" Creates one-hot encoding for the input. No embedding is used as of now\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(self.inputs_placeholder)[0]\n",
    "        embedding = tf.one_hot(self.inputs_placeholder, self.config.num_classes)\n",
    "        embedding = tf.reshape(embedding ,[self.config.batch_size,self.config.max_length,\n",
    "                                           self.config.feature_size*self.config.num_classes])\n",
    "        return embedding\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "\n",
    "        \"\"\" Get the input from the embedding layer\n",
    "        \"\"\"\n",
    "        x = self.add_embedding()\n",
    "\n",
    "        \"\"\" Create a RNN first & define a placeholder for the initial state\n",
    "        \"\"\"\n",
    "        if self.config.model_type == \"gru\":\n",
    "            cell = tf.nn.rnn_cell.GRUCell(self.config.hidden_size)\n",
    "        elif self.config.model_type == \"rnn\":\n",
    "            cell = tf.nn.rnn_cell.BasicRNNCell(self.config.hidden_size)\n",
    "        else:\n",
    "            raise Exception(\"Unsuppoprted model type...\")\n",
    "\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.dropout_placeholder)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * self.config.num_layers, state_is_tuple=False)\n",
    "\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        dynamic_max_length = tf.shape(x)[1] \n",
    "        zero_state = cell.zero_state(batch_size, tf.float32)\n",
    "        self.in_state = tf.placeholder_with_default(zero_state, [None, cell.state_size])\n",
    "\n",
    "        \"\"\" First find the sequence length and then use it to run the model\n",
    "        \"\"\"\n",
    "        #length = tf.reduce_sum(tf.reduce_max(tf.sign(x), 2), 1)\n",
    "        output, self.out_state = tf.nn.dynamic_rnn(cell, x, initial_state=self.in_state)\n",
    "        output = tf.reshape(output, shape=[-1, self.config.hidden_size])\n",
    "\n",
    "        \"\"\" Pass it through a linear + Softmax layer to get the predictions\n",
    "        \"\"\"\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        W = tf.get_variable(\"W\", shape=[self.config.hidden_size, self.config.num_classes], initializer=xavier_init )\n",
    "        b1 = tf.get_variable(\"b1\", shape=[self.config.num_classes], initializer=xavier_init )\n",
    "        preds = tf.add(tf.matmul(output,W),b1)\n",
    "        preds = tf.reshape(preds, shape=[batch_size,dynamic_max_length, self.config.num_classes])\n",
    "        return preds\n",
    "\n",
    "    def add_loss_op(self, preds):\n",
    "        loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels_placeholder, logits=preds) )\n",
    "        scaled_loss = loss/np.log(2)\n",
    "        tf.summary.scalar('loss', scaled_loss);\n",
    "        return scaled_loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "        \"\"\"\n",
    "        global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.config.lr)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return global_step, train_op\n",
    "\n",
    "    def loss_on_batch(self, sess, inputs_batch, labels_batch, initial_state=None):\n",
    "        feed = self.create_feed_dict(inputs_batch=inputs_batch, labels_batch=labels_batch, initial_state=initial_state, keep_prob=1.0)\n",
    "        loss, out_state = sess.run([self.loss,self.out_state], feed_dict=feed)\n",
    "        return loss, out_state\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch, initial_state=None, dropout=1.0):\n",
    "        feed = self.create_feed_dict(inputs_batch=inputs_batch, labels_batch=labels_batch, initial_state=initial_state, keep_prob=dropout)\n",
    "        _, loss,out_state,_step, summary = sess.run([self.train_op, self.loss, self.out_state, self.global_step, self.merged_summaries], feed_dict=feed)\n",
    "        return loss, out_state, _step, summary\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for text in open('../../Data/Original Data/input_info.txt'):\n",
    "    print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = 'abc'\n",
    "u.index('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, vocab):\n",
    "    return [vocab.index(x) for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader():\n",
    "    window = 4\n",
    "    delay = 3\n",
    "    vocab = 'abcdef'\n",
    "    text = 'abcdefabcdefabc'\n",
    "    text = encode(text, vocab)\n",
    "    for start in range(len(text) - 2 * window +1):\n",
    "        chunk = text[start: start + 2 * window]\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    stream = reader()\n",
    "    feature_size = 3\n",
    "    window = 4\n",
    "    for element in stream:\n",
    "        print element\n",
    "        input_batch = []\n",
    "        label_batch = []\n",
    "        _input = []\n",
    "        for start in range(window):\n",
    "            _input.append(element[start:start+feature_size])\n",
    "        input_batch.append(_input)\n",
    "        label_batch.append(element[feature_size+1:feature_size+window+1])\n",
    "        for i, num in enumerate(input_batch):\n",
    "            print input_batch[i]\n",
    "            print label_batch[i]\n",
    "            print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 0, 1]\n",
      "[[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5]]\n",
      "[4, 5, 0, 1]\n",
      "\n",
      "[1, 2, 3, 4, 5, 0, 1, 2]\n",
      "[[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 0]]\n",
      "[5, 0, 1, 2]\n",
      "\n",
      "[2, 3, 4, 5, 0, 1, 2, 3]\n",
      "[[2, 3, 4], [3, 4, 5], [4, 5, 0], [5, 0, 1]]\n",
      "[0, 1, 2, 3]\n",
      "\n",
      "[3, 4, 5, 0, 1, 2, 3, 4]\n",
      "[[3, 4, 5], [4, 5, 0], [5, 0, 1], [0, 1, 2]]\n",
      "[1, 2, 3, 4]\n",
      "\n",
      "[4, 5, 0, 1, 2, 3, 4, 5]\n",
      "[[4, 5, 0], [5, 0, 1], [0, 1, 2], [1, 2, 3]]\n",
      "[2, 3, 4, 5]\n",
      "\n",
      "[5, 0, 1, 2, 3, 4, 5, 0]\n",
      "[[5, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, 4]]\n",
      "[3, 4, 5, 0]\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 0, 1]\n",
      "[[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5]]\n",
      "[4, 5, 0, 1]\n",
      "\n",
      "[1, 2, 3, 4, 5, 0, 1, 2]\n",
      "[[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 0]]\n",
      "[5, 0, 1, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.dynamic_rnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
